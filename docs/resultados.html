<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="pt" xml:lang="pt"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.269">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Resolvendo Captchas - 3&nbsp; Resultados</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 1em;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./conclusoes.html" rel="next">
<link href="./metodologia.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "Nenhum resultado",
    "search-matching-documents-text": "documentos correspondentes",
    "search-copy-link-title": "Copiar link para a busca",
    "search-hide-matches-text": "Esconder correspondências adicionais",
    "search-more-match-text": "mais correspondência neste documento",
    "search-more-matches-text": "mais correspondências neste documento",
    "search-clear-button-title": "Limpar",
    "search-detached-cancel-button-title": "Cancelar",
    "search-submit-button-title": "Enviar"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<meta name="citation_title" content="[[3]{.chapter-number}&nbsp; [Resultados]{.chapter-title}]{#sec-results .quarto-section-identifier}">
<meta name="citation_language" content="pt">
<meta name="citation_reference" content="citation_title=Recognizing objects in adversarial clutter: Breaking a visual CAPTCHA;,citation_author=Greg Mori;,citation_author=Jitendra Malik;,citation_publication_date=2003;,citation_cover_date=2003;,citation_year=2003;,citation_volume=1;,citation_conference=IEEE;">
<meta name="citation_reference" content="citation_title=Recognizing objects in adversarial clutter: Breaking a visual CAPTCHA;,citation_author=Greg Mori;,citation_author=Jitendra Malik;,citation_publication_date=2003;,citation_cover_date=2003;,citation_year=2003;,citation_volume=1;,citation_conference=IEEE;">
<meta name="citation_reference" content="citation_title=Designing human friendly human interaction proofs (HIPs);,citation_author=Kumar Chellapilla;,citation_author=Kevin Larson;,citation_author=Patrice Simard;,citation_author=Mary Czerwinski;,citation_publication_date=2005-04-02;,citation_cover_date=2005-04-02;,citation_year=2005;,citation_fulltext_html_url=https://doi.org/10.1145/1054972.1055070;,citation_doi=10.1145/1054972.1055070;,citation_conference=Association for Computing Machinery;,citation_series_title=CHI ’05;">
<meta name="citation_reference" content="citation_title=Using machine learning to break visual human interaction proofs (HIPs);,citation_author=Kumar Chellapilla;,citation_author=Patrice Simard;,citation_publication_date=2004;,citation_cover_date=2004;,citation_year=2004;,citation_volume=17;,citation_journal_title=Advances in neural information processing systems;">
<meta name="citation_reference" content="citation_title=Multi-digit number recognition from street view imagery using deep convolutional neural networks;,citation_author=Ian J. Goodfellow;,citation_author=Yaroslav Bulatov;,citation_author=Julian Ibarz;,citation_author=Sacha Arnoud;,citation_author=Vinay Shet;,citation_publication_date=2013;,citation_cover_date=2013;,citation_year=2013;,citation_journal_title=arXiv preprint arXiv:1312.6082;">
<meta name="citation_reference" content="citation_title=Deep learning;,citation_author=Yann LeCun;,citation_author=Yoshua Bengio;,citation_author=Geoffrey Hinton;,citation_publication_date=2015;,citation_cover_date=2015;,citation_year=2015;,citation_issue=7553;,citation_volume=521;,citation_journal_title=nature;">
<meta name="citation_reference" content="citation_title=Deep learning;,citation_author=Yann LeCun;,citation_author=Yoshua Bengio;,citation_author=Geoffrey Hinton;,citation_publication_date=2015;,citation_cover_date=2015;,citation_year=2015;,citation_issue=7553;,citation_volume=521;,citation_journal_title=nature;">
<meta name="citation_reference" content="citation_title=Generative adversarial networks;,citation_author=Ian Goodfellow;,citation_author=Jean Pouget-Abadie;,citation_author=Mehdi Mirza;,citation_author=Bing Xu;,citation_author=David Warde-Farley;,citation_author=Sherjil Ozair;,citation_author=Aaron Courville;,citation_author=Yoshua Bengio;,citation_publication_date=2020;,citation_cover_date=2020;,citation_year=2020;,citation_issue=11;,citation_volume=63;,citation_journal_title=Communications of the ACM;">
<meta name="citation_reference" content="citation_title=Generative Adversarial Networks;,citation_author=Ian J. Goodfellow;,citation_author=Jean Pouget-Abadie;,citation_author=Mehdi Mirza;,citation_author=Bing Xu;,citation_author=David Warde-Farley;,citation_author=Sherjil Ozair;,citation_author=Aaron Courville;,citation_author=Yoshua Bengio;,citation_doi=10.48550/arXiv.1406.2661;">
<meta name="citation_reference" content="citation_title=A generative vision model that trains with high data efficiency and breaks text-based CAPTCHAs;,citation_author=Dileep George;,citation_author=Wolfgang Lehrach;,citation_author=Ken Kansky;,citation_author=Miguel Lázaro-Gredilla;,citation_author=Christopher Laan;,citation_author=Bhaskara Marthi;,citation_author=Xinghua Lou;,citation_author=Zhaoshi Meng;,citation_author=Yi Liu;,citation_author=Huayan Wang;,citation_publication_date=2017;,citation_cover_date=2017;,citation_year=2017;,citation_issue=6368;,citation_volume=358;,citation_journal_title=Science;">
<meta name="citation_reference" content="citation_title=Yet another text captcha solver: A generative adversarial network based approach;,citation_author=Guixin Ye;,citation_author=Zhanyong Tang;,citation_author=Dingyi Fang;,citation_author=Zhanxing Zhu;,citation_author=Yansong Feng;,citation_author=Pengfei Xu;,citation_author=Xiaojiang Chen;,citation_author=Zheng Wang;,citation_publication_date=2018;,citation_cover_date=2018;,citation_year=2018;">
<meta name="citation_reference" content="citation_title=Yet another text captcha solver: A generative adversarial network based approach;,citation_author=Guixin Ye;,citation_author=Zhanyong Tang;,citation_author=Dingyi Fang;,citation_author=Zhanxing Zhu;,citation_author=Yansong Feng;,citation_author=Pengfei Xu;,citation_author=Xiaojiang Chen;,citation_author=Zheng Wang;,citation_publication_date=2018;,citation_cover_date=2018;,citation_year=2018;">
<meta name="citation_reference" content="citation_title=Make complex captchas simple: a fast text captcha solver based on a small number of samples;,citation_author=Yao Wang;,citation_author=Yuliang Wei;,citation_author=Mingjin Zhang;,citation_author=Yang Liu;,citation_author=Bailing Wang;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_volume=578;,citation_journal_title=Information Sciences;">
<meta name="citation_reference" content="citation_title=A survey of CAPTCHA technologies to distinguish between human and computer;,citation_author=Xin Xu;,citation_author=Lei Liu;,citation_author=Bo Li;,citation_publication_date=2020;,citation_cover_date=2020;,citation_year=2020;,citation_volume=408;,citation_journal_title=Neurocomputing;">
<meta name="citation_reference" content="citation_title=Adversarial examples: Attacks and defenses for deep learning;,citation_author=Xiaoyong Yuan;,citation_author=Pan He;,citation_author=Qile Zhu;,citation_author=Xiaolin Li;,citation_publication_date=2019;,citation_cover_date=2019;,citation_year=2019;,citation_issue=9;,citation_volume=30;,citation_journal_title=IEEE transactions on neural networks and learning systems;">
<meta name="citation_reference" content="citation_title=Regularizing deep neural networks by noise: Its interpretation and optimization;,citation_author=Hyeonwoo Noh;,citation_author=Tackgeun You;,citation_author=Jonghwan Mun;,citation_author=Bohyung Han;,citation_publication_date=2017;,citation_cover_date=2017;,citation_year=2017;,citation_volume=30;,citation_journal_title=Advances in Neural Information Processing Systems;">
<meta name="citation_reference" content="citation_title=reCAPTCHA: Human-Based Character Recognition via Web Security Measures;,citation_author=Luis Ahn;,citation_author=Benjamin Maurer;,citation_author=Colin McMillen;,citation_author=David Abraham;,citation_author=Manuel Blum;,citation_publication_date=2008-09-12;,citation_cover_date=2008-09-12;,citation_year=2008;,citation_fulltext_html_url=https://www.science.org/doi/10.1126/science.1160379;,citation_issue=5895;,citation_doi=10.1126/science.1160379;,citation_volume=321;,citation_language=en;,citation_journal_title=Science;">
<meta name="citation_reference" content="citation_title=Reinforcement learning: An introduction;,citation_author=Richard S. Sutton;,citation_author=Andrew G. Barto;,citation_publication_date=2018;,citation_cover_date=2018;,citation_year=2018;">
<meta name="citation_reference" content="citation_title=Deep Generative Positive-Unlabeled Learning under Selection Bias;,citation_author=Byeonghu Na;,citation_author=Hyemi Kim;,citation_author=Kyungwoo Song;,citation_author=Weonyoung Joo;,citation_author=Yoon-Yeong Kim;,citation_author=Il-Chul Moon;,citation_publication_date=2020-10-19;,citation_cover_date=2020-10-19;,citation_year=2020;,citation_fulltext_html_url=https://doi.org/10.1145/3340531.3411971;,citation_doi=10.1145/3340531.3411971;,citation_conference=Association for Computing Machinery;,citation_series_title=CIKM ’20;">
<meta name="citation_reference" content="citation_title=Análise de sobrevivência aplicada;,citation_author=Enrico Antonio Colosimo;,citation_author=Suely Ruiz Giolo;,citation_publication_date=2006;,citation_cover_date=2006;,citation_year=2006;">
<meta name="citation_reference" content="citation_title=A brief introduction to weakly supervised learning;,citation_author=Zhi-Hua Zhou;,citation_publication_date=2018;,citation_cover_date=2018;,citation_year=2018;,citation_issue=1;,citation_volume=5;,citation_journal_title=National science review;">
<meta name="citation_reference" content="citation_title=Computing machinery and intelligence;,citation_author=Alan M. Turing;,citation_publication_date=2009;,citation_cover_date=2009;,citation_year=2009;,citation_inbook_title=undefined;">
<meta name="citation_reference" content="citation_title=Telling humans and computers apart automatically or how lazy cryptographers do AI (Tech. Rep. No. CMU-CS-02-117);,citation_author=L. Ahn;,citation_author=M. Blum;,citation_author=J. Langford;,citation_publication_date=2002;,citation_cover_date=2002;,citation_year=2002;,citation_fulltext_html_url=http://reports-archive.adm.cs.cmu.edu/anon/2002/CMU-CS-02-117.pdf;">
<meta name="citation_reference" content="citation_title=Inaccessibility of CAPTCHA;,citation_fulltext_html_url=https://www.w3.org/TR/turingtest/;">
<meta name="citation_reference" content="citation_title=Estado brasileiro e transparência avaliando a aplicação da Lei de Acesso à Informação;,citation_author=Gregory Michener;,citation_author=Luiz Fernando Moncau;,citation_author=Rafael Braem Velasco;,citation_publication_date=2015;,citation_cover_date=2015;,citation_year=2015;">
<meta name="citation_reference" content="citation_title=Open data in science;,citation_author=Peter Murray-Rust;,citation_publication_date=2008;,citation_cover_date=2008;,citation_year=2008;,citation_journal_title=Nature Precedings;">
<meta name="citation_reference" content="citation_title=Observatório da insolvência: Rio de Janeiro;,citation_fulltext_html_url=https://abj.org.br/pesquisas/obsrjrj/;">
<meta name="citation_reference" content="citation_title=Tempo dos processos relacionados à adoção;,citation_fulltext_html_url=https://abj.org.br/pesquisas/adocao/;">
<meta name="citation_reference" content="citation_title=Diagnóstico do Contencioso Tributário Administrativo;,citation_fulltext_html_url=https://abj.org.br/pesquisas/bid-tributario/;">
<meta name="citation_reference" content="citation_title=Diagnóstico do Contencioso Tributário Administrativo;,citation_fulltext_html_url=https://abj.org.br/pesquisas/bid-tributario/;">
<meta name="citation_reference" content="citation_title=Web scraping;,citation_author=Bo Zhao;,citation_publication_date=2017;,citation_cover_date=2017;,citation_year=2017;,citation_fulltext_html_url=https://www.researchgate.net/profile/Bo-Zhao-3/publication/317177787_Web_Scraping/links/5c293f85a6fdccfc7073192f/Web-Scraping.pdf;,citation_journal_title=Encyclopedia of big data;">
<meta name="citation_reference" content="citation_title=A brief introduction to weakly supervised learning;,citation_author=Zhi-Hua Zhou;,citation_publication_date=2018;,citation_cover_date=2018;,citation_year=2018;,citation_issue=1;,citation_volume=5;,citation_journal_title=National science review;">
<meta name="citation_reference" content="citation_title=Semi-supervised learning literature survey;,citation_author=Xiaojin Jerry Zhu;,citation_publication_date=2005;,citation_cover_date=2005;,citation_year=2005;">
<meta name="citation_reference" content="citation_title=A note on learning from multiple-instance examples;,citation_author=Avrim Blum;,citation_author=Adam Kalai;,citation_publication_date=1998;,citation_cover_date=1998;,citation_year=1998;,citation_issue=1;,citation_volume=30;,citation_journal_title=Machine learning;">
<meta name="citation_reference" content="citation_title=Learning with multiple labels;,citation_author=Rong Jin;,citation_author=Zoubin Ghahramani;,citation_publication_date=2002;,citation_cover_date=2002;,citation_year=2002;,citation_volume=15;,citation_journal_title=Advances in neural information processing systems;">
<meta name="citation_reference" content="citation_title=Gradient-based learning applied to document recognition;,citation_author=Yann LeCun;,citation_author=Léon Bottou;,citation_author=Yoshua Bengio;,citation_author=Patrick Haffner;,citation_publication_date=1998;,citation_cover_date=1998;,citation_year=1998;,citation_issue=11;,citation_volume=86;,citation_journal_title=Proceedings of the IEEE;">
<meta name="citation_reference" content="citation_title=Generalized linear models;,citation_author=John Ashworth Nelder;,citation_author=Robert WM Wedderburn;,citation_publication_date=1972;,citation_cover_date=1972;,citation_year=1972;,citation_issue=3;,citation_volume=135;,citation_journal_title=Journal of the Royal Statistical Society: Series A (General);">
<meta name="citation_reference" content="citation_title=Hierarchical Text-Conditional Image Generation with CLIP Latents;,citation_author=Aditya Ramesh;,citation_author=Prafulla Dhariwal;,citation_author=Alex Nichol;,citation_author=Casey Chu;,citation_author=Mark Chen;,citation_doi=10.48550/arXiv.2204.06125;">
<meta name="citation_reference" content="citation_title=Feature engineering and selection: A practical approach for predictive models;,citation_author=Max Kuhn;,citation_author=Kjell Johnson;,citation_publication_date=2019;,citation_cover_date=2019;,citation_year=2019;">
<meta name="citation_reference" content="citation_title=Understanding dropout;,citation_author=Pierre Baldi;,citation_author=Peter J. Sadowski;,citation_publication_date=2013;,citation_cover_date=2013;,citation_year=2013;,citation_volume=26;,citation_journal_title=Advances in neural information processing systems;">
<meta name="citation_reference" content="citation_title=A review on ensembles for the class imbalance problem: bagging-, boosting-, and hybrid-based approaches;,citation_author=Mikel Galar;,citation_author=Alberto Fernandez;,citation_author=Edurne Barrenechea;,citation_author=Humberto Bustince;,citation_author=Francisco Herrera;,citation_publication_date=2011;,citation_cover_date=2011;,citation_year=2011;,citation_issue=4;,citation_volume=42;,citation_journal_title=IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews);">
<meta name="citation_reference" content="citation_title=Adam: A Method for Stochastic Optimization;,citation_author=Diederik P. Kingma;,citation_author=Jimmy Ba;,citation_doi=10.48550/arXiv.1412.6980;">
<meta name="citation_reference" content="citation_title=Efficient backprop;,citation_author=Yann A. LeCun;,citation_author=Léon Bottou;,citation_author=Genevieve B. Orr;,citation_author=Klaus-Robert Müller;,citation_publication_date=2012;,citation_cover_date=2012;,citation_year=2012;,citation_inbook_title=undefined;">
<meta name="citation_reference" content="citation_title=Learning from partial labels;,citation_author=Timothee Cour;,citation_author=Ben Sapp;,citation_author=Ben Taskar;,citation_publication_date=2011;,citation_cover_date=2011;,citation_year=2011;,citation_volume=12;,citation_journal_title=The Journal of Machine Learning Research;">
<meta name="citation_reference" content="citation_title=Provably consistent partial-label learning;,citation_author=Lei Feng;,citation_author=Jiaqi Lv;,citation_author=Bo Han;,citation_author=Miao Xu;,citation_author=Gang Niu;,citation_author=Xin Geng;,citation_author=Bo An;,citation_author=Masashi Sugiyama;,citation_publication_date=2020;,citation_cover_date=2020;,citation_year=2020;,citation_volume=33;,citation_journal_title=Advances in Neural Information Processing Systems;">
<meta name="citation_reference" content="citation_title=Logistic regression for partial labels;,citation_author=Yves Grandvalet;,citation_publication_date=2002;,citation_cover_date=2002;,citation_year=2002;">
<meta name="citation_reference" content="citation_title=Learning from ambiguously labeled examples;,citation_author=Eyke Hüllermeier;,citation_author=Jürgen Beringer;,citation_publication_date=2006;,citation_cover_date=2006;,citation_year=2006;,citation_issue=5;,citation_volume=10;,citation_journal_title=Intelligent Data Analysis;">
<meta name="citation_reference" content="citation_title=A conditional multinomial mixture model for superset label learning;,citation_author=Liping Liu;,citation_author=Thomas Dietterich;,citation_publication_date=2012;,citation_cover_date=2012;,citation_year=2012;,citation_volume=25;,citation_journal_title=Advances in neural information processing systems;">
<meta name="citation_reference" content="citation_title=Learning from complementary labels;,citation_author=Takashi Ishida;,citation_author=Gang Niu;,citation_author=Weihua Hu;,citation_author=Masashi Sugiyama;,citation_publication_date=2017;,citation_cover_date=2017;,citation_year=2017;,citation_volume=30;,citation_journal_title=Advances in neural information processing systems;">
<meta name="citation_reference" content="citation_title=Learning from complementary labels;,citation_author=Takashi Ishida;,citation_author=Gang Niu;,citation_author=Weihua Hu;,citation_author=Masashi Sugiyama;,citation_publication_date=2017;,citation_cover_date=2017;,citation_year=2017;,citation_volume=30;,citation_journal_title=Advances in neural information processing systems;">
<meta name="citation_reference" content="citation_title=magick: Advanced Graphics and Image-Processing in R;,citation_author=Jeroen Ooms;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_fulltext_html_url=https://CRAN.R-project.org/package=magick;">
<meta name="citation_reference" content="citation_title=magick: Advanced Graphics and Image-Processing in R;,citation_author=Jeroen Ooms;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_fulltext_html_url=https://CRAN.R-project.org/package=magick;">
<meta name="citation_reference" content="citation_title=Learning with biased complementary labels;,citation_author=Xiyu Yu;,citation_author=Tongliang Liu;,citation_author=Mingming Gong;,citation_author=Dacheng Tao;,citation_publication_date=2018;,citation_cover_date=2018;,citation_year=2018;">
<meta name="citation_reference" content="citation_title=Learning with multiple complementary labels;,citation_author=Lei Feng;,citation_author=Takuo Kaneko;,citation_author=Bo Han;,citation_author=Gang Niu;,citation_author=Bo An;,citation_author=Masashi Sugiyama;,citation_publication_date=2020;,citation_cover_date=2020;,citation_year=2020;,citation_conference=PMLR;">
<meta name="citation_reference" content="citation_title=Web scraping;,citation_author=Bo Zhao;,citation_publication_date=2017;,citation_cover_date=2017;,citation_year=2017;,citation_fulltext_html_url=https://www.researchgate.net/profile/Bo-Zhao-3/publication/317177787_Web_Scraping/links/5c293f85a6fdccfc7073192f/Web-Scraping.pdf;,citation_journal_title=Encyclopedia of big data;">
<meta name="citation_reference" content="citation_title=xml2: Parse XML;,citation_author=Hadley Wickham;,citation_author=Jim Hester;,citation_author=Jeroen Ooms;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_fulltext_html_url=https://CRAN.R-project.org/package=xml2;">
<meta name="citation_reference" content="citation_title=stringr: Simple, Consistent Wrappers for Common String Operations;,citation_author=Hadley Wickham;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_fulltext_html_url=https://CRAN.R-project.org/package=stringr;">
<meta name="citation_reference" content="citation_title=rvest: Easily Harvest (Scrape) Web Pages;,citation_author=Hadley Wickham;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_fulltext_html_url=https://CRAN.R-project.org/package=rvest;">
<meta name="citation_reference" content="citation_title=Captcha and Its Techniques: A Review;,citation_author=Kiranjot Kaur;,citation_author=Sunny Behal;,citation_publication_date=2014-01-01;,citation_cover_date=2014-01-01;,citation_year=2014;,citation_volume=5;,citation_journal_title=International Journal of Computer Science and Information Technologies,;">
<meta name="citation_reference" content="citation_title=Multivariate binomial/multinomial control chart;,citation_author=Jian Li;,citation_author=Fugee Tsung;,citation_author=Changliang Zou;,citation_publication_date=2014;,citation_cover_date=2014;,citation_year=2014;,citation_issue=5;,citation_volume=46;,citation_journal_title=IIE Transactions;">
<meta name="citation_reference" content="citation_title=Batch normalization: Accelerating deep network training by reducing internal covariate shift;,citation_author=Sergey Ioffe;,citation_author=Christian Szegedy;,citation_publication_date=2015;,citation_cover_date=2015;,citation_year=2015;,citation_conference=PMLR;">
<meta name="citation_reference" content="citation_title=torch: Tensors and Neural Networks with ’GPU’ Acceleration;,citation_author=Daniel Falbel;,citation_author=Javier Luraschi;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_fulltext_html_url=https://CRAN.R-project.org/package=torch;">
<meta name="citation_reference" content="citation_title=luz: Higher Level ’API’ for ’torch’;,citation_author=Daniel Falbel;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_fulltext_html_url=https://CRAN.R-project.org/package=luz;">
<meta name="citation_reference" content="citation_title=torchvision: Models, Datasets and Transformations for Images;,citation_author=Daniel Falbel;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_fulltext_html_url=https://CRAN.R-project.org/package=torchvision;">
<meta name="citation_reference" content="citation_title=R: A Language and Environment for Statistical Computing;,citation_author=R Core Team;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_fulltext_html_url=https://www.R-project.org/;">
<meta name="citation_reference" content="citation_title=Feature engineering and selection: A practical approach for predictive models;,citation_author=Max Kuhn;,citation_author=Kjell Johnson;,citation_publication_date=2019;,citation_cover_date=2019;,citation_year=2019;">
<meta name="citation_reference" content="citation_title=tensorflow: R Interface to ’TensorFlow’;,citation_author=JJ Allaire;,citation_author=Yuan Tang;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_fulltext_html_url=https://CRAN.R-project.org/package=tensorflow;">
<meta name="citation_reference" content="citation_title=decryptr: An extensible API for breaking captchas;,citation_author=Julio Trecenti;,citation_author=Caio Lente;,citation_author=Daniel Falbel;,citation_author=Milene Farhat;,citation_author=Beatriz Vianna;,citation_author=Evelin Angelica;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;">
<meta name="citation_reference" content="citation_title=reticulate: Interface to ’Python’;,citation_author=Kevin Ushey;,citation_author=JJ Allaire;,citation_author=Yuan Tang;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_fulltext_html_url=https://CRAN.R-project.org/package=reticulate;">
<meta name="citation_reference" content="citation_title=torch: Tensors and Neural Networks with ’GPU’ Acceleration;,citation_author=Daniel Falbel;,citation_author=Javier Luraschi;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_fulltext_html_url=https://CRAN.R-project.org/package=torch;">
<meta name="citation_reference" content="citation_title=piggyback: Managing Larger Data on a GitHub Repository;,citation_author=Carl Boettiger;,citation_author=Tan Ho;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_fulltext_html_url=https://CRAN.R-project.org/package=piggyback;">
<meta name="citation_reference" content="citation_title=usethis: Automate Package and Project Setup;,citation_author=Hadley Wickham;,citation_author=Jennifer Bryan;,citation_author=Malcolm Barrett;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_fulltext_html_url=https://CRAN.R-project.org/package=usethis;">
<meta name="citation_reference" content="citation_title=piggyback: Managing Larger Data on a GitHub Repository;,citation_author=Carl Boettiger;,citation_author=Tan Ho;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_fulltext_html_url=https://CRAN.R-project.org/package=piggyback;">
<meta name="citation_reference" content="citation_title=Hierarchical Text-Conditional Image Generation with CLIP Latents;,citation_author=Aditya Ramesh;,citation_author=Prafulla Dhariwal;,citation_author=Alex Nichol;,citation_author=Casey Chu;,citation_author=Mark Chen;,citation_doi=10.48550/arXiv.2204.06125;">
<meta name="citation_reference" content="citation_title=AVA: A large-scale database for aesthetic visual analysis;,citation_author=Naila Murray;,citation_author=Luca Marchesotti;,citation_author=Florent Perronnin;,citation_publication_date=2012;,citation_cover_date=2012;,citation_year=2012;,citation_conference=IEEE;">
</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Resultados</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Resolvendo Captchas</a> 
        <div class="sidebar-tools-main">
    <a href="./Resolvendo-Captchas.pdf" title="Download PDF" class="sidebar-tool px-1"><i class="bi bi-file-pdf"></i></a>
</div>
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">Sobre este documento</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./introducao.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introdução</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./metodologia.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Metodologia</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./resultados.html" class="sidebar-item-text sidebar-link active"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Resultados</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./conclusoes.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Conclusões</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./bibliografia.html" class="sidebar-item-text sidebar-link">Bibliografia</a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">Apêndices</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./pacote.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Pacotes</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Índice</h2>
   
  <ul>
  <li><a href="#sec-discussao" id="toc-sec-discussao" class="nav-link active" data-scroll-target="#sec-discussao">Discussão</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span id="sec-results" class="quarto-section-identifier d-none d-lg-block"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Resultados</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<div>
<blockquote class="blockquote">
<p>I’m not a robot, I’m a human. But I’m pretty sure the robot is better at this than I am.</p>
<p>– ChatGPT</p>
</blockquote>
</div>
<!-- DUAS ALTERNATIVAS PARA OS RESULTADOS -->
<!-- Essa aqui é a primeira opção, se eu conseguir obter os resultados teóricos com maior detalhamento a tempo -->
<!-- Neste capítulo, discute-se os resultados da metodologia WAWL. Para isso, são apresentadas tanto resultados matemáticos quanto empíricos que demostram que os a proposta possui bons resultados. -->
<!-- O capítulo foi organizado três seções. Na @sec-result-theory, são apresentadas as propriedades matemáticas e probabilísticas da estratégia adotada. Na @sec-result-sim, são revelados os resultados empíricos obtidos das simulações. Na @sec-discussao, os resultados são discutidos, fazendo a ponte entre as hipóteses de pesquisa e os resultados. -->
<!-- ## Resultados teóricos {#sec-result-theory} -->
<!-- Nesta seção, buscou-se demonstrar que o uso dos dados fornecidos pelo oráculo com adaptação da função de perda i) não piora o poder preditivo do modelo e ii) converge para o modelo preditivo ótimo. Para isso, é necessário retomar algumas definições para avançar. -->
<!-- ## Resultados empíricos {#sec-result-sim} -->
<p>Neste capítulo, discute-se os resultados da metodologia WAWL. Para isso, são apresentadas resultados empíricos que demostram que os a proposta possui bons resultados.</p>
<div class="cell">

</div>
<p>Os resultados foram obtidos a partir das simulações com diversos Captchas. Foram realizadas 65 simulações no total, variando no tipo de Captcha, a acurácia do modelo inicial e a quantidade de tentativas no oráculo.</p>
<p>Para realizar os cálculos, montou-se uma base de dados com os resultados das simulações. A base está disponível publicamente no <a href="https://github.com/jtrecenti/doutorado">repositório da tese</a> e contém informações do Captcha ajustado (<code>captcha</code>), da quantidade de observações do modelo inicial (<code>n</code>), da quantidade de tentativas do oráculo (<code>ntry</code>), da etapa de simulação (<code>fase</code>, inicial ou WAWL), do caminho do modelo ajustado (<code>model</code>) e da acurácia obtida (<code>acc</code>).</p>
<p>Os resultados gerais mostram um ganho de 333% na acurácia após a aplicação da metodologia WAWL. Ou seja, em média, a acurácia do modelo no terceiro passo da simulação (ver <a href="metodologia.html#sec-modelo-final"><span>Seção&nbsp;2.4.3</span></a>) foi de mais de <strong>três vezes</strong> a acurácia do modelo inicial. Em termos absolutos (diferença entre as acurácias), o ganho foi de 33%, ou seja, após o terceiro passo modelos ganharam, em média, 33% de acurácia.</p>
<div class="cell">

</div>
<p>As <a href="#fig-simulacao-geral-ntry-relativo">Figuras&nbsp;<span>3.1</span></a> e <a href="#fig-simulacao-geral-ntry-absoluto"><span>3.2</span></a> mostram os ganhos relativos e absolutos, separando os resultados gerais por quantidade de tentativas. Cada ponto é o resultado de uma simulação e o ponto em destaque é o valor médio, acompanhado de intervalo <span class="math inline">\(m \mp 2*s/\sqrt(n)\)</span>, com <span class="math inline">\(m\)</span> sendo a média, <span class="math inline">\(s\)</span> o desvio padrão e <span class="math inline">\(n\)</span> a quantidade de dados. A linha pontilhada indica se a acurácia aumentou ou diminuiu após a aplicação da técnica.</p>
<p>Na <a href="#fig-simulacao-geral-ntry-relativo">Figura&nbsp;<span>3.1</span></a>, é possível notar que os ganhos em acurácia apresentam alta variabilidade, mas que apresentam uma tendência positiva com relação ao número de tentativas. O ganho entre aplicar 5 e 10 tentativas é menos expressivo do que o ganho entre aplicar 1 e 5 tentativas, indicando que a oportunidade oferecida por sites que aceitam vários chutes é relevante e que não há necessidade de realizar tantos chutes para aproveitar essa oportunidade.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-simulacao-geral-ntry-relativo" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="resultados_files/figure-html/fig-simulacao-geral-ntry-relativo-1.png" class="img-fluid figure-img" style="width:60.0%"></p>
<p></p><figcaption class="figure-caption">Figura&nbsp;3.1: Ganho percentual ao utilizar a técnica do oráculo, dividido por quantidade de tentativas.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>A Figura <a href="#fig-simulacao-geral-ntry-absoluto">Figura&nbsp;<span>3.2</span></a>, com as os ganhos absolutos, mostra a mesma informação mas em quantidades mais fáceis de interpretar. O ganho médio absoluto em sites que permitem mais de um chute ficou em torno de 40%, enquanto que o ganho com apenas um chute ficou um pouco acima de 25%. Importante notar também que o uso do oráculo só piorou a acurácia do model em casos que com apenas um chute, mostrando que a técnica é efetiva de forma consistente.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-simulacao-geral-ntry-absoluto" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="resultados_files/figure-html/fig-simulacao-geral-ntry-absoluto-1.png" class="img-fluid figure-img" style="width:60.0%"></p>
<p></p><figcaption class="figure-caption">Figura&nbsp;3.2: Ganhos absolutos ao utilizar a técnica do oráculo, dividido por quantidade de tentativas.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<div class="cell">

</div>
<p>As <a href="#fig-simulacao-geral-inicial-relativo">Figuras&nbsp;<span>3.3</span></a> e <a href="#fig-simulacao-geral-inicial-absoluto"><span>3.4</span></a> apresentam os resultados gerais separando por acurácia inicial do modelo. A estrutura do gráfico é similar às visualizações anteriores, que separaram os resultados por quantidade de tentativas. As categorias escolhidas foram: até 10%, mais de 10% até 35% e mais de 35% de acurácia no modelo inicial. A escolha dos intervalos se deram pela quantidade de observações em cada categoria.</p>
<p>A <a href="#fig-simulacao-geral-inicial-relativo">Figura&nbsp;<span>3.3</span></a> mostra os ganhos relativos. É possível notar uma tendência de queda no ganho de acurácia com uso do oráculo conforme aumenta a acurácia do modelo inicial. Esse resultado é esperado, pois, como a acurácia é um número entre zero e um, um modelo que já possui alta acurácia não tem a possibilidade de aumentar muito de forma absoluta.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-simulacao-geral-inicial-relativo" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="resultados_files/figure-html/fig-simulacao-geral-inicial-relativo-1.png" class="img-fluid figure-img" style="width:60.0%"></p>
<p></p><figcaption class="figure-caption">Figura&nbsp;3.3: Ganho percentual ao utilizar a técnica do oráculo, dividido por acurácia do modelo inicial.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>A <a href="#fig-simulacao-geral-inicial-absoluto">Figura&nbsp;<span>3.4</span></a> mostra os ganhos absolutos. O gráfico apresenta o mesmo problema que o anterior, já que o ganho máximo depende da acurácia inicial do modelo. Ainda assim, é possível notar que, em termos absolutos, modelos com acurácia inicial entre 10% e 35% apresentaram um ganho maior que modelos com acurácia inicial de até 10%.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-simulacao-geral-inicial-absoluto" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="resultados_files/figure-html/fig-simulacao-geral-inicial-absoluto-1.png" class="img-fluid figure-img" style="width:60.0%"></p>
<p></p><figcaption class="figure-caption">Figura&nbsp;3.4: Ganho absoluto ao utilizar a técnica do oráculo, dividido por acurácia do modelo inicial.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>Para lidar com o fato da acurácia ser um número limitado, fizemos o mesmo gráficos de antes, mas ajustado pelo máximo possível que a técnica do oráculo poderia proporcionar. O ganho absoluto ajustado de uma simulação é dado por</p>
<p><span class="math display">\[
\text{ganho} = \frac{\text{oráculo } - \text{ inicial}}{1\; - \text{ inicial}}.
\]</span> A <a href="#fig-simulacao-geral-inicial-absoluto-ajustado">Figura&nbsp;<span>3.5</span></a> mostra os ganhos ajustados. Pelo gráfico, é possível notar que existe um ganho expressivo da técnica WAWL do oráculo para modelos iniciais com mais de 10% de acurácia com relação a modelos iniciais com até 10% de acurácia. Ou seja, quando o modelo inicial é fraco, o ganho ao usar a técnica é menor. É importante notar, no entanto, que as simulações mostram a aplicação da técnica apenas uma vez – é possível baixar mais dados e atualizar o modelo indefinidamente. O menor efeito da técnica para modelos iniciais fracos não significa, portanto, que a técnica não funciona para modelos iniciais fracos; pelo contrário: ela ajuda o modelo a sair do estado inicial e o leva para um estado com acurácia maior, de onde seria possível aplicar a técnica novamente para obter resultads mais expressivos.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-simulacao-geral-inicial-absoluto-ajustado" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="resultados_files/figure-html/fig-simulacao-geral-inicial-absoluto-ajustado-1.png" class="img-fluid figure-img" style="width:60.0%"></p>
<p></p><figcaption class="figure-caption">Figura&nbsp;3.5: Ganho absoluto ao utilizar a técnica do oráculo, dividido por acurácia do modelo inicial.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>Na <a href="#fig-simulacao-captcha">Figura&nbsp;<span>3.6</span></a>, são apresentados os resultados separando por Captcha. Cada linha é uma combinação de Captcha, quantidade de tentativas e acurácia modelo inicial, classificados nas três categorias mostradas anteriormente. As linhas pontilhadas indicam modelos ajustados com mais de uma tentativa, enquanto as linhas contínuas mostram modelos ajustados com apenas uma tentativa. A primeira extremidade de cada linha, do lado esquerdo, indica a acurácia do modelo inicial e a segunda extremidade, do lado direito, a acurácia do modelo usando o método WAWL.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-simulacao-captcha" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="resultados_files/figure-html/fig-simulacao-captcha-1.png" class="img-fluid figure-img" style="width:100.0%"></p>
<p></p><figcaption class="figure-caption">Figura&nbsp;3.6: Resultados da simulação por captcha, quantidade de tentativas e modelo inicial.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>Pelo gráfico, é possível identificar duas informações relevantes. Como já verificado anteriormente, os modelos ajustados com mais de uma tentativa apresentam maiores ganhos do que os modelos ajustados com apenas uma tentativa. Verifica-se também que modelos com acurácia inicial de até 10% só apresentam ganhos menores que os modelos com acurácia inicial maior que 10% nos casos em que apenas uma tentativa é definda. Ou seja, existe interação entre a quantidade de tentativas e a acurácia do modelo inicial ao avaliar o impacto nos ganhos empíricos do método WAWL.</p>
<p>Pelos resultados das simulações, é possível concluir que o método WAWL foi bem sucedido. Primeiro, o método apresenta resultados expressivos e de forma consistente, sem realizar novas anotações manuais. Além disso, a técnica aproveita a oportunidade oferecida pelos sites de obter o <em>feedback</em> oráculo múltiplas vezes na mesma imagem. Finalmente, o método apresenta, em média, resultados positivos mesmo para modelos iniciais muito fracos (com acurácia de até 10%), indicando que sua aplicação é possível para qualquer modelo inicial, o que é bastante factível de atingir com bases pequenas ou com modelos generalistas para resolver Captchas.</p>
<!-- ### Aplicação iterada -->
<p>Um possível problema em aplicar o WAWL é que a técnica poderia introduzir viés de seleção no modelo, impedindo-o de ser aprimorado indefinidamente. Mesmo que os resultados teóricos dêem uma boa base para acreditar que isso não seja verdade, foi feito um experimento adicional, com apenas um dos Captchas, para verificar se a aplicação da técnica múltiplas vezes apresenta bons resultados.</p>
<p>O Captcha escolhido para a simulação foi o <code>trf5</code>, por ser um Captcha que não aceita múltiplos chutes, em uma tentativa de obter um pior caso. Para esse Captcha, o melhor modelo obtido com a técnica do oráculo foi considerado como modelo inicial, sendo usado para baixar novos dados do site do Tribunal. Os novos dados foram adicionados à base de treino, ajustando-se um novo modelo.</p>
<p>A <a href="#fig-aplicacao-iterada">Figura&nbsp;<span>3.7</span></a> mostra os resultados da aplicação iterada. A utilização da técnica não só funcionou como levou o modelo a uma acurácia de 100%.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-aplicacao-iterada" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="resultados_files/figure-html/fig-aplicacao-iterada-1.png" class="img-fluid figure-img" style="width:60.0%"></p>
<p></p><figcaption class="figure-caption">Figura&nbsp;3.7: Resultados da aplicação iterada da técnica.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>O resultado sugere que a técnica pode ser utilizada em várias iterações para auxiliar no aprendizado do modelo. Ela sugere, ainda, que uma técnica de aprendizado ativo com <em>feedback</em> automático do oráculo pode dar bons resultados, já que a forma de obter os dados não introduz viés de seleção no ajuste do modelo.</p>
<section id="sec-discussao" class="level2">
<h2 class="anchored" data-anchor-id="sec-discussao">Discussão</h2>
<p>Os resultados apresentados nas seções anteriores mostram que o método WAWL possui boas propriedades matemáticas e bons resultados empíricos. Nesta seção, os resultados foram confrontados com as hipóteses de pesquisa definidos na <a href="introducao.html#sec-hipoteses"><span>Seção&nbsp;1.6</span></a> de forma crítica.</p>
<p>A primeira hipótese de pesquisa diz respeito à pertinência de utilizar do aprendizado fracamente supervisionado como forma de ajustar modelos para resolver Captchas. A hipótese foi verificada, tanto do ponto de vista teórico quanto do ponto de vista prático.</p>
<p>Na parte teórica, várias pesquisas já apontam que o aprendizado com rótulos parciais ou rótulos complementares têm boas propriedades. Por isso, já seria esperado que uma nova função de perda, desde que pensada com cuidado, traria resultados positivos. O resultado foi verificado a partir da obtenção das propriedades da função de perda proposta, mostrando que a função de risco baseada nessa perda se aproxima da função de risco de um problema completamente supervisionado.</p>
<p>Na parte prática, até o momento não existiam evidências de que a utilização de rótulos parciais ou rótulos complementares teriam bons resultados empíricos em Captchas. Isso foi verificado em todos os 12 Captchas estudados, sendo 10 obtidos do mundo real. Em todos os casos, a função de perda proposta funcionou bem e trouxe ganhos significativos na acurácia do modelo, tanto em termos relativos quanto absolutos. Isso demonstra que a escolha do método se alia bem ao problema que deu origem à pesquisa, que são os Captchas.</p>
<p>Sobre a parte de aplicação iterada do WAWL, cabe um comentário. O resultado encontrado, com 100% de acurácia, pode sugerir que o método WAWL sempre chegará em um resultado de 100% para qualquer Captcha que surgir. No entanto, pode ser que exista uma limitação na capacidade do modelo, que é habilidade do modelo para se ajustar aos dados a partir dos parâmetros. Pode ser que a arquitetura de rede neural escolhida para resolver o Captcha não seja capaz de chegar a um modelo com 100% de acurácia, independente da quantidade de imagens observadas. É importante olhar o resultado apresentado de forma crítica e compreender que os resultados finais podem ser limitados, já que a arquitetura da rede neural não é parte do método WAWL.</p>
<p>A segunda hipótese de pesquisa é a possibilidade de aliar a área de raspagem de dados com a área de modelagem estatística. Essa parte está bem mais relacionada com a parte prática da pesquisa, já que os conceitos de raspagem de dados não são utilizados para estudar a propriedade dos modelos. A hipótese também foi verificada, já que o método WAWL, que utiliza técnicas de raspagem de dados, apresentou bons resultados empíricos.</p>
<p>Neste momento, cabe um comentário sobre o ineditismo da ponte entre raspagem de dados e estatística. É verdade que existem muitas pesquisas que são possibilitadas por conta dos dados obtidos via raspagem de dados: as pesquisas da ABJ, mencionadas na <a href="introducao.html#sec-captchas-publicos"><span>Seção&nbsp;1.1</span></a> são alguns exemplos. Também existem soluções que utilizam dados provenientes de raspagem de dados para construção de modelos: por exemplo, o <a href="https://openai.com/dall-e-2/">DALL-E-2</a>, que é parte de uma base de dados construída utilizando imagens baixadas da internet <span class="citation" data-cites="ramesh murray2012">(<a href="bibliografia.html#ref-murray2012" role="doc-biblioref">MURRAY; MARCHESOTTI; PERRONNIN, 2012</a>; <a href="bibliografia.html#ref-ramesh" role="doc-biblioref">RAMESH et al., [s.d.]</a>)</span>. No entanto, até o momento da realização da pesquisa, não foi encontrado nenhum trabalho que utiliza a raspagem de dados como parte do processo de aprendizado estatístico. O método WAWL conecta as áreas de forma intrínseca, podendo ser entendida como uma nova variação de aumentação de dados aplicada a redes neurais convolucionais.</p>
<p>O fato da raspagem de dados ser relevante para o ajuste de um modelo estatístico pode levar a algumas discussões sobre o ensino da estatística. Primeiro, é importante mencionar que:</p>
<ol type="1">
<li>Raspagem de dados não faz parte dos currículos de Bacharelado em Estatística das principais universidades do país<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>. Logo, pode-se argumentar que raspagem de dados não é uma área de interesse da estatística.</li>
<li>Raspagem de dados não é uma área de conhecimento bem definida, como álgebra ou análise de sobrevivência. A área é melhor desenvolvida através de aplicações práticas e utilização de ferramentas (como R ou python) do que através de aulas teóricas.</li>
</ol>
<p>Os resultados levam, então, a um problema de balanceamento entre pertinência e oportunidade. De um lado, a área de raspagem não se encaixa muito bem no currículo de estatística. Por outro lado, a área expande as possibilidades de atuação de uma profissional da estatística.</p>
<p>Para aliar a pertinência e a oportunidade, uma opção seria oferecer disciplinas optativas de raspagem de dados nos cursos de estatística. Para aumentar a quantidade de potenciais ministrantes, a disciplina poderia ser oferecida em parceria com outros cursos, como ciência da computação, matemática aplicada e engenharias. Dessa forma, as pessoas interessadas teriam a oportunidade de aprender um pouco sobre as técnicas principais, conectando a raspagem de dados com as áreas de conhecimento específicas, como é o caso do Captcha, que alia raspagem de dados com estatística e inteligência artificial.</p>
<p>No final, as duas hipóteses de pesquisa foram verificadas. No processo de obtenção dos resultados, no entanto, um terceiro avanço muito importante foi realizado na parte computacional. O pacote <code>{captcha}</code> e os pacotes auxiliares <code>{captchaDownload}</code> e <code>{captchaOracle}</code> são frutos desse trabalho. Pela primeira vez, foi construída uma ferramenta aberta contendo um fluxo de trabalho adaptado para trabalhar com Captchas. Além disso, trata-se de uma das primeiras aplicações completas dos pacotes <code>{torch}</code> e <code>{luz}</code>, que têm potencial de revolucionar a forma em que os modelos estatísticos são desenvolvidos por pessoas que fazem pesquisa em estatística. Os pacotes foram descritos em detalhes no sec-pacote.</p>
<p>Por fim, todos os modelos construídos foram disponibilizados no pacote <code>{captcha}</code>. Os códigos, dados e resultados das simulações estão disponíveis no pacote <code>{captchaOracle}</code>. Os dados utilizados para elaboraçõ da tese estão disponíveis no <a href="https://github.com/jtrecenti/doutorado">repositório da tese no GitHub</a>. Dessa forma, a pesquisa pode ser considerada como reprodutível, podendo servir como base para pesquisas futuras.</p>


<div id="refs" class="references csl-bib-body" data-entry-spacing="1" role="doc-bibliography" style="display: none">
<div id="ref-murray2012" class="csl-entry" role="doc-biblioentry">
MURRAY, N.; MARCHESOTTI, L.; PERRONNIN, F. <strong>AVA: A large-scale database for aesthetic visual analysis</strong>. IEEE, 2012.
</div>
<div id="ref-ramesh" class="csl-entry" role="doc-biblioentry">
RAMESH, A. et al. <a href="https://doi.org/10.48550/arXiv.2204.06125">Hierarchical Text-Conditional Image Generation with CLIP Latents</a>. [s.d.].
</div>
</div>
</section>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr>
<ol>
<li id="fn1"><p>Universidades consultadas: USP Butantã (IME), UFSCar, UNESP, Unicamp, USP São Carlos (ICMC), UFBA, UFPR, UFRGS, UFPE, UFAM, UFRN, UFF, ENCE, UFRJ, UFMG, UnB e UFG.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copiada");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copiada");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./metodologia.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Metodologia</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./conclusoes.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Conclusões</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>